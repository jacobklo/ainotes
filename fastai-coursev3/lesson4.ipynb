{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem on classifying movie reviews\n",
    "\n",
    "For movie reviews, if we just traditional method of classification, where we use each text occurrences as a input feature, and output will be positive/negative, this will not have good result.\n",
    "\n",
    "Because language is very complex. E.g. sarcasm, hidden message, etc. We need a model that can \"link\" diffferent worlds together and form a new meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson4/lesson4-1.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "Language Model has a very specific meaning in NLP. It is a model that learns to predict the next world of a sentence. To predict the next world of a sentence, you actually need to learn quite a lot about a language, and also quite a lot of world knowledge.\n",
    "\n",
    "E.g. The language model should be about to predict:\n",
    "```\n",
    "I'd like to eat a hot ___. (dog)\n",
    "It was a hot ___. (day)\n",
    "```\n",
    "It can not mix 2 words between these sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engram\n",
    "\n",
    "It is basically means how oven a pair of words or triplets of words tends to appear next to each other.\n",
    "\n",
    "An engram is terrible to solve text related classifier. Because it only account for pair or triplets of words, it cannot solve the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create language model\n",
    "\n",
    "So how do we create a language model that can answer the example above? we need a pre-trained model that trained a large set of data, such as wikipedia.\n",
    "\n",
    "It is like getting a model to learn English by asking it to read all wikipedia page.\n",
    "\n",
    "Note: Wikitext103 has over 1 billion tokens, while movie reviews probably only has 2000 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create language model for movie reviews\n",
    "\n",
    "We can now use the pre-trained wikitext103 lanugage model, and transfer it to learn a new model, which using the movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
    "\n",
    "- we need to take care of punctuation\n",
    "- some words are contractions of two different words, like isn't or don't\n",
    "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
    "\n",
    "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson4/lesson4-2.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
    "- the \"'s\" are grouped together in one token\n",
    "- the contractions are separated like this: \"did\", \"n't\"\n",
    "- content has been cleaned for any HTML symbol and lower cased\n",
    "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalization\n",
    "\n",
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
    "\n",
    "The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string). \n",
    "\n",
    "Note: every word in the vocabularies is going to require a separate row in the weight matrix, that's why we restrict to 60000 max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson3-imdb.ipynb language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For langugae model training, we can use all the data we have, in train, even test and validation test set. \n",
    "\n",
    "Because we are not using label, we are not training a classifier. This is just a language model.  There is no \"cheating\" involve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson3-imdb.ipynb Classifier\n",
    "\n",
    "Note: when we create a new classifer out from a pre-trained language model, make sure the vocabolaries is exactly the same. (the token). \n",
    "\n",
    "That's why we pass the vocab from language model to classifer:\n",
    "```\n",
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Learning rate, slice, use 2.6 as magic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
