{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem on classifying movie reviews\n",
    "\n",
    "For movie reviews, if we just traditional method of classification, where we use each text occurrences as a input feature, and output will be positive/negative, this will not have good result.\n",
    "\n",
    "Because language is very complex. E.g. sarcasm, hidden message, etc. We need a model that can \"link\" diffferent worlds together and form a new meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson4/lesson4-1.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "Language Model has a very specific meaning in NLP. It is a model that learns to predict the next world of a sentence. To predict the next world of a sentence, you actually need to learn quite a lot about a language, and also quite a lot of world knowledge.\n",
    "\n",
    "E.g. The language model should be about to predict:\n",
    "```\n",
    "I'd like to eat a hot ___. (dog)\n",
    "It was a hot ___. (day)\n",
    "```\n",
    "It can not mix 2 words between these sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engram\n",
    "\n",
    "It is basically means how oven a pair of words or triplets of words tends to appear next to each other.\n",
    "\n",
    "An engram is terrible to solve text related classifier. Because it only account for pair or triplets of words, it cannot solve the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create language model\n",
    "\n",
    "So how do we create a language model that can answer the example above? we need a pre-trained model that trained a large set of data, such as wikipedia.\n",
    "\n",
    "It is like getting a model to learn English by asking it to read all wikipedia page.\n",
    "\n",
    "Note: Wikitext103 has over 1 billion tokens, while movie reviews probably only has 2000 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create language model for movie reviews\n",
    "\n",
    "We can now use the pre-trained wikitext103 lanugage model, and transfer it to learn a new model, which using the movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
    "\n",
    "- we need to take care of punctuation\n",
    "- some words are contractions of two different words, like isn't or don't\n",
    "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
    "\n",
    "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson4/lesson4-2.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
    "- the \"'s\" are grouped together in one token\n",
    "- the contractions are separated like this: \"did\", \"n't\"\n",
    "- content has been cleaned for any HTML symbol and lower cased\n",
    "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalization\n",
    "\n",
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
    "\n",
    "The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string). \n",
    "\n",
    "Note: every word in the vocabularies is going to require a separate row in the weight matrix, that's why we restrict to 60000 max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson3-imdb.ipynb language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For langugae model training, we can use all the data we have, in train, even test and validation test set. \n",
    "\n",
    "Because we are not using label, we are not training a classifier. This is just a language model.  There is no \"cheating\" involve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson3-imdb.ipynb Classifier\n",
    "\n",
    "Note: when we create a new classifer out from a pre-trained language model, make sure the vocabolaries is exactly the same. (the token). \n",
    "\n",
    "That's why we pass the vocab from language model to classifer:\n",
    "```\n",
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Learning rate, slice, use 2.6 as magic number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "\n",
    "variables that is finite. like marital-status, occupation.\n",
    "\n",
    "We need to use a technique called Embedding to put those values into nerual net.\n",
    "\n",
    "Sometimes you need to think carefully about what to put as ategorical variable. E.g. Day of the month, 1st, 15th, 30th of day probably has different purchasing behavior. Futhermore, there are only 31 of days in a month. (Cardinality is not high ) Isn't this better to use then continuous variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variables\n",
    "\n",
    "It is numbers, range of numbers, has infinite digits.\n",
    "\n",
    "These data can directly put into the nerual net, like pixels in a image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastai Processor\n",
    "\n",
    "It is data pre-processing, They run once before any training. The run once on the training set, and then any kind of state or metadata is shared across test and validation set.\n",
    "\n",
    "E.g. For image recognition, we have set of class for different cat breeds, and turned into a number, they are the same between training set and validation set.\n",
    "\n",
    "```\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastai Transformation\n",
    "\n",
    "It is data argumentation, like randomizing it. It will be different everytime we want to train again. Like image transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data validation\n",
    "\n",
    "Rule of thumb is to split a validation set in a continuous manner. For time related data, should split a validation set by a range of period; For video, should be a validation set of continuous frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai metrics\n",
    "\n",
    "Those are for print-outs, does not change the result\n",
    "```\n",
    "learn = tabular_learner(data, layers=[200,100], metrics=accuracy)\n",
    "learn.fit(1, 1e-2)\n",
    "Total time: 00:03\n",
    "epoch\ttrain_loss\tvalid_loss\taccuracy\n",
    "1\t0.354604\t0.378520\t0.820000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson4-tabular.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "![a](lesson4/lesson4-3.png \"\")\n",
    "\n",
    "Collaborative filtering is basically movie reviews. We have users and movies, and users have rating on any number of movies. \n",
    "\n",
    "The goal here is for a given user, with some rating already in the data, to predict a rating that the user did not have rating yet ( did not watch yet ). Like fill in the blanks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrix\n",
    "\n",
    "The example above has very few blanks, but it real life, it will probably has a lot of blanks.\n",
    "\n",
    "a lot of blanks is a sparse matrix. If we store users and movies in a matrix, which is a sparse matrix ( not enough data ), it will be wasting space. Because the space complexity is o(n^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold start problem\n",
    "\n",
    "When there is a lot of empty data ( which form a sparse matrix ), it is hard to predict. It is called cold start problem.\n",
    "\n",
    "It is really a case-by-case basis. But for example, netflix will ask the new user what movies did they watch. By doing so, netflix get some info on the new user, so it solve the cold start problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson4/lesson4-4.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basically idea of solving collaborative filtering is, for each user, we have generated 5 random column of number, do the same for earh movie.\n",
    "\n",
    "Then, for each rating, we perform matrix multiplication for it, with respect to 5 numbers from user and movies.\n",
    "```\n",
    "H25 =IF(H2=\"\",0,MMULT($B25:$F25,H$19:H$23))\n",
    "```\n",
    "\n",
    "Then, we use gradient descent to run thorugh all the batches, and tweak those numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "This is just the 5 columns and rows for user in above example.\n",
    "So that example has a embedding matrix of shape[15, 5]\n",
    "Also has another embedding matrix for movies of shape [15, 5]\n",
    "\n",
    "the 5 is user choice, you can have as many dimention you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "So, just do a dot product of the 5 numbers from embedding matrix user to the ones in movies is not enough. What happen if a movie is more popular? What happen if the user likes to rating movie higher than average (give all 5 stars )?\n",
    "\n",
    "So we need a new number called bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit calculation output\n",
    "\n",
    "![a](lesson4/lesson4-5.png \"\")\n",
    "\n",
    "Although the collaborative filter model above can already predict the user rating output, we can do one thing better, by limit the model output from 0 to 5 only ( 0 to 5 stars).\n",
    "\n",
    "We can do this by using a sigmoid function at the end of the layer.\n",
    "\n",
    "This save a lot of calculations, and give a much better result.\n",
    "\n",
    "We actually want to y_range to be a little higher than 5, because a sigmoid function will never hit 5.\n",
    "\n",
    "```\n",
    "learn = collab_learner(data, n_factors=40, y_range=[0,5.5], wd=1e-1)\n",
    "```\n",
    "Check out the implementation from fastai class EmbeddingDotBias.\n",
    "![a](lesson4/lesson4-6.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How data is stored\n",
    "\n",
    "The example above is using a matrix to store ratings, where row is users and column is movies.\n",
    "But we cannot store data that way, consider there can be millions of users\n",
    "\n",
    "We store it as an arrayof [userid, movieid, rating]\n",
    "\n",
    "![a](lesson4/lesson4-7.png \"\")\n",
    "\n",
    "We added a new user_index and movie_index to make it easier to think.\n",
    "\n",
    "We will still have the embedding matrix, store as follow:\n",
    "![a](lesson4/lesson4-8.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "To quickly get the row of embeddings number ( 5 numbers in this case ) for each user, we can use a identity matrix and do a matrix multiplication\n",
    "\n",
    "![a](lesson4/lesson4-9.png \"\")\n",
    "\n",
    "Do the same for movies:\n",
    "\n",
    "![a](lesson4/lesson4-10.png \"\")\n",
    "\n",
    "Now, we can do a dot prodct of User activation and Movie activation, and get a prediction.\n",
    "\n",
    "![a](lesson4/lesson4-11.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "You can quickly see that both the user activation and movie activations, the number is the same as the embedding matrix. This make sense, because we are just multiply it with identity matrix.\n",
    "\n",
    "So, we can actually just do a \"array lookup\", to look at the index of the embedding for user/movie, and put it in the activations\n",
    "```\n",
    "M3 =OFFSET($B$3,$L3,M$2)\n",
    "```\n",
    "![a](lesson4/lesson4-12.png \"\")\n",
    "\n",
    "Embedding in here means an array lookup, because in here it is identical to matrix multiplcation of an identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent features/ Latent Factors\n",
    "\n",
    "![a](lesson4/lesson4-13.png \"\")\n",
    "\n",
    "So, for a particular user, who like an actor A, and like a particular movie B, which have that actor A in it; After training, the embeddings ( especially the highlighted purple ) is going to have to get some features out of the data.\n",
    "\n",
    "For example, the number in the purple box eventually is going to represent a certain thing, even though the model does not know that user \"like actor A\" specifically, but it knows user like this feature.\n",
    "\n",
    "The model will find hidden features, even though we did not specifically program the model to find \"actor A movies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "![a](lesson4/lesson4-13.png \"\")\n",
    "\n",
    "So, what happen if a user like actor A, and there is a movie that have A in it, but that movie is really bad?\n",
    "\n",
    "The previous model would just recommend anyway.\n",
    "\n",
    "So we need to add a bias. So now it will not solidly depend on only 1 feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson4-collab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricinple Componenets Analysis\n",
    "\n",
    "It basically take a larger dimention of tensor and try to fit the data in lower dimentions\n",
    "```\n",
    "movie_pca = movie_w.pca(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
