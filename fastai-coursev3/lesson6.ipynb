{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to make all the features as clear for the neural network to pick up as possible.\n",
    "\n",
    "E.g. If we only have 1 feature of date, it is hard for neural network to pick up, is a pattern happen on weekend.\n",
    "\n",
    "So, we can pre-process and create a new feature called is_weekend.\n",
    "\n",
    "![](lesson6/lesson6-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai preprocessor - Categorify\n",
    "![](lesson6/lesson6-2.png)\n",
    "\n",
    "Some of the values in the panda dataframe is a string of multiple values, this preprocessor is going to internally convert those into numbers.\n",
    "\n",
    "E.g. \"Jan,Apr,Jul,Oct\" was a string, and it is converted to list of numbers that store internally.\n",
    "\n",
    "This get all the categories shows in this column\n",
    "```\n",
    "small_train_df.PromoInterval.cat.categories\n",
    "```\n",
    "\n",
    "This show the internal number\n",
    "```\n",
    "small_train_df['PromoInterval'].cat.codes\n",
    "```\n",
    "-1 in codes means NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai preprocessor - fill_missing\n",
    "![](lesson6/lesson6-3.png)\n",
    "Create a new boolean feature, and identify TRUE for missing data in FEATURE_na; false for otherwise\n",
    "\n",
    "For the original data that is missing, it will put median into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai - treat label as continuous variable explicitly\n",
    "\n",
    "We can use label_cls=FloatList, to treat label as continuous variable explicitly. On the other hand, Fastai will treat integer data as categorical variable.\n",
    "```\n",
    "data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)\n",
    "                .split_by_idx(valid_idx)\n",
    "                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)\n",
    "                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))\n",
    "                .databunch())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Percentage Error\n",
    "![](lesson6/lesson6-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Log\n",
    "\n",
    "The above example pass the label and perform log(y) immediately (log=True). We use Log when the label have long-tail distribution, like population, the amount of sales. We use when we care about the percentage differences rather than exact differences.\n",
    "\n",
    "That's why the loss function of the above example is RMSPE, instead of RMSE.\n",
    "\n",
    "It is claimed that doing a log(y) on input ( so yi now is log(yi) ), the RMSPE becomes RMSE in math, I don't know yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "![](lesson6/lesson6-5.png)\n",
    "\n",
    "Dropout will throw out some activations on each layer at random, for each batch, by given a probability. On next batch, put those back and it will be some other activation that get throw out on each layer.\n",
    "\n",
    "It means that no one activation can kind of memorize some part of the input, because that's what happen when we overfit. When we overfit, some part of the model is basically learning to recognize a particular image, rather a feature in general.\n",
    "```\n",
    "ps=[0.001,0.01]\n",
    "ps=0.01\n",
    "```\n",
    "![](lesson6/lesson6-6.png)\n",
    "At testing, we will always use all nodes ( turn off droupout ).\n",
    "\n",
    "It also say you should mutiply the weights by p, since now that all the nodes are present. The weights will not be accurate ( since some weights are dropped before ).\n",
    "\n",
    "Pytorch already did those for us, so we did not need to care about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check lesson6 on Dropout\n",
    "[Jump to Lesson6 Dropout](https://youtu.be/hkBa9pU-H48?t=2328)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "[Jump to Lesson6 on Batch Norm](https://youtu.be/hkBa9pU-H48?t=2670)\n",
    "\n",
    "![](lesson6/lesson6-7.png)\n",
    "The algorithm is going to take a mini-batch, and since BatchNorm is a layer, so the things come into it is an activation { x1, x2, ... xm }.\n",
    "\n",
    "We find the mean of those mini-batch μ.\n",
    "\n",
    "Then we find the variance of those mini-batch σ.\n",
    "\n",
    "Then we normailize and have x_hat.\n",
    "\n",
    "But turns out those are not the most important steps.\n",
    "\n",
    "Then we take those values and add a bias called β. We also add another thing γ, which is a lot like a bias, but we multiply it with x_hat. β and γ are learnable parameters. These are things that learn with gradient descent.\n",
    "\n",
    "![](lesson6/lesson6-8.png)\n",
    "There are 2 papers to prove that Batch norm did not actually help reducing internal coveriate shift.\n",
    "\n",
    "In the above picture, you can see the red line ( Standard ) is very bumpy, while the blue line (standard + BatchNorm ) is not bumpy at all. What it means is you can increase the learning rate using batchNorm. Because those big bumps represent the times you are really at risk of your set of weights jumping off into some awful part of the weight space that it can never get out of again. So if BatchNorm get you less bumpy, then you can train at a higher learning rate. ( So it is why it train faster ).\n",
    "\n",
    "### Then Why BatchNorm has so better and faster result?\n",
    "![](lesson6/lesson6-9.png)\n",
    "In the above picture, y_hat is the result of some functions f of our various weights, can be millions of them, and also include our input layers x. This function is the nerual net.\n",
    "\n",
    "L is the loss function.\n",
    "\n",
    "Our current training model ( up the this y_hat activation ) is outputing range of -1 to 1. Let say for movie review, we want the output to be range of 1 to 5, so the output is way off where they need to be, both scale and position. We can train the network and eventually it will learn to increase the weights so the y_hat have a closer output to expected, but it is very hard to do ( train a long time ) because all these weights interact in very intricate ways ( the neural net is complex, connected network between nodes )\n",
    "\n",
    "But, batchnorm provaide a new two bias β and γ ( g and b in above drawing ). Now, we can increase scale very easily ( incrase γ ), and also increase positiom to where it needs to be ( incrase β ). β and γ has a direct gradient to incrase the scale, change the mean, directly through this BatchNorm Layer. No need to go through the complex nerual network of layers.\n",
    "\n",
    "So basically, batchnorm makes it easy to shift the output up and down, and in and out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Norm\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "It is basically transform your original data and have multiple set of new data, which is trasform in different ways.\n",
    "\n",
    "![](lesson6/lesson6-10.png)\n",
    "\n",
    "Check out the transformaiton library for image in fastai:\n",
    "\n",
    "[Fastai Vision Transform](https://docs.fast.ai/vision.transform.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Kernal\n",
    "![](lesson6/lesson6-11.png)\n",
    "\n",
    "So, we have a 3x3 matrix we called kernal. For each 3x3 pixels in the image ( except the edge pixels), we multiply the pixel with the kernal, and form a new image. The image has 1 less pixel on each edge.\n",
    "\n",
    "For this particular kernal, the newly formed image now outlining the horizontal edges.\n",
    "\n",
    "This is called convolution.\n",
    "\n",
    "Check out this website:\n",
    "[Image Kernal](http://setosa.io/ev/image-kernels/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "So now we can use this newly formed image in the first layer and use it with another kernal that highlight left edge, the result is a second layer that good at finding top-left corner stuff. Thats the idea of CNN.\n",
    "\n",
    "![](lesson6/lesson6-12.png)\n",
    "![](lesson6/lesson6-13.png)\n",
    "\n",
    "The above 2 picture shows that convolution is just matrix multiplcation.\n",
    "\n",
    "![](lesson6/lesson6-14.png)\n",
    "We need to think about padding. because the most edge cannot be multiply by the kernal 3x3 ( so let say we want to comput image(0,0), but the center of a 3x3 kernal is outside pixel image(0,0). So we need to pad zeros on the edges of image, for the output to have the same size as the input image.\n",
    "\n",
    "![](lesson6/lesson6-15.png)\n",
    "\n",
    "So, normally image will have 3 channel, red, blue and green. Our input pixel will be h * w * 3. (height x width x channel ).\n",
    "\n",
    "Since there are 3 channel, we cannot just use a 3x3 kernal. It is 2D. and it also doesn't make sense to reuse the same 3x3 kernal for each channel. \n",
    "\n",
    "So we now have a 3x3x3 kernal.\n",
    "\n",
    "The input images with h * w * 3, multiply with 3*3*3 kernal, will ouput h * w newly formed image, which is 2D.\n",
    "\n",
    "It doesn't really can do much with just one 2D activation.\n",
    "\n",
    "So, we have another different 3*3*3 kernal, in this case 16 of the kernals in total.\n",
    "\n",
    "So the output newly formed activations is now h * w * 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride 2 Convolution\n",
    "\n",
    "![](lesson6/lesson6-16.png)\n",
    "Tranditional CNN, which kernal is calculated for each pixel, in each layer, is very computational heavy.\n",
    "\n",
    "Stride 2 Convolution, is basically doing the same thing, but only for every other pixel.\n",
    "\n",
    "This will result in outputing a activation shape of h/2 * w/2 * channel.\n",
    "\n",
    "We overcome this somewhat by twice the size of kernal, so new output activation will have h/2 * w/2 * 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson6-pets-more.ipynb #Convolution-kernel for doing convolution manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average pooling\n",
    "![](lesson6/lesson6-17.png)\n",
    "\n",
    "After going deeper down the layers in CNN, the activations now has a shape of example of 11 * 11 * 512 ( does have to be exact 11, just example ). We can calculate the mean of each 11*11 plane, have output of an array of means which has size of 512.\n",
    "\n",
    "So, if we predicted this particular image is a Maine_Coon cat, which highlighted in yellow, it has to have an high value in that yellow box in output array.\n",
    "\n",
    "Walking backward, the only way we get a high value there, is with the matrix multiplication of 512 * 37.\n",
    "\n",
    "it's going to represent a simple weighted linear combination of all of the 512 values here.\n",
    "\n",
    "So if we are going to be able to say I'm pretty confident that this is a Maine Coon, just by taking the weighted sum of a bunch of inputs, those inputs are going to have to represent features like how fluffy is it, what color is its nose, how long are its legs, all kinds of things that can be used, etc. \n",
    "\n",
    "Because for the other thing which figured out is this is a bulldog, It's going to use exactly the same kind of 512 inputs with a different set of weights because that's all a matrix multiplication is. It is just a bunch of weighted sums, a different weighted sum for each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Activations\n",
    "![](lesson6/lesson6-18.png)\n",
    "Extand from above explanation, we know that in this CNN, potentially dozens or even hundreds of layers of convolution, it must have eventually come up with an 11 * 11 face for each of these features ( hidden in 11 * 11 * 512 tensor ).\n",
    "\n",
    "It is saying in this blue box here, how much is that part of the image like a pointy ear, how much is it fluffy, how much is it like a long leg.\n",
    "\n",
    "So each face is what we call each of these represents a different features.\n",
    "\n",
    "So, instead of doing what average pooling does, if we do a average across all 512 faces, for each grid point in 11 by 11 plane, we can get a output result of 11 by 11 place, showing how \"activated\" of each grid point is.\n",
    "\n",
    " When it came to figuring out that if this was a maine coon, how many signs of maine coon-ishess was there in that part of the 11 by 11 grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap\n",
    "![](lesson6/lesson6-19.png)\n",
    "It is a plot of that average activations.\n",
    "\n",
    "### Check lesson6-pets-more.ipynb #Heatmap for drawing heatmap manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics and Data Science\n",
    "[Jump to Lesson 6 on Ethics](https://youtu.be/hkBa9pU-H48?t=6546)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
