{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lessaib 5 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs\n",
    "- Weights/parameters\n",
    "    - Random\n",
    "- Activations\n",
    "- Activation functions / nonlinearities\n",
    "- Output\n",
    "- Loss\n",
    "- Metric\n",
    "- Cross-entropy\n",
    "- Softmax\n",
    "- Fine tuning\n",
    "    - Layer deletion and random weights\n",
    "    - Freezing & unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-1.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "It is the features.\n",
    "In this case, MNIST input is handwriting pictures of 28 by 28 pixels. Sine we use each pixel as a feature, so the input shape will be 28 * 28 = 784\n",
    "\n",
    "\n",
    "It is one colume of 784 rows.\n",
    "\n",
    "If it is a color picture, which mean it may have 3 channel, so the input feature will be 28 * 28 * 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "For matrix multiplication to happen correctly, one dimention of shape has to match, so the row of the weighs is fixed to the row of inputs, which is 784\n",
    "\n",
    "Note: the input layer is probably transform into shape of [1, 784]\n",
    "\n",
    "The other dimention is you pick really. Depends on want you want to outcome activation is, can be 1, can be 5, 10, 1 million. In this picture, we picked 5.\n",
    "\n",
    "That's the part what a neural networks is, how many neurons you want in this layer.\n",
    "\n",
    "Note: the last activation, the output, should be the dimention you need. For MNIST, we want to predict number from 0 to 10, so we will have dimention shape of [ 10, 1 ]. So we should pick 10 at the last layer.\n",
    "\n",
    "This weights is initialized as random number, but also should be normalized. Check out kaming normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "The new layer of activation is the result of a matrix multication between last activations and weights is performed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-2.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "This is a function that modify each value in the activation. ( element-wise function )\n",
    "\n",
    "ReLU repalce every negative number to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat next layer\n",
    "\n",
    "The process is repeat next layer, where having a new set of weights, this time is 8.\n",
    "\n",
    "Just remember that the last layer should be 10, for the example. So your last layer of weights should second-last len(activation) and 10 column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer\n",
    "\n",
    "It is just steps, can be a step of activation functions, can be a matrix multication of another weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-3.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Layer\n",
    "\n",
    "This is simply the output, which is a activation. \n",
    "\n",
    "But most of the time, we can also have another activation function, sometimes a sigmoid function, the calculate the probability of the output. \n",
    "\n",
    "E.g. the output layer has these output for MNIST by predicting what number of this handwriting should be:\n",
    "\n",
    "```\n",
    "[[0.123], [0.232], [0.897], [0.123], [0.123], [0.232], [0.456], [0.123], [0.232], [0.345], [0.254]]\n",
    "```\n",
    "\n",
    "See that the 2nd value is high, so the neural network has predicted that the handwriting should be 2\n",
    "\n",
    "We use sigmoid function to 'cap' the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost Function\n",
    "\n",
    "It is a function to calculate how accurate the network predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squred Error\n",
    "\n",
    "For Mean-Squred Error, it is just Sum of (y-y_hat)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation theorem\n",
    "\n",
    "It is basically saying if there are big-enough data, for a enough complex of function ( think of it as a huge multi-layer neural net ), you can solve any arbitrariy high level of accuracy for that function ( the basis of machine learning ). One important thing is that we need to give some non-linearity to these functions.\n",
    "\n",
    "Otherwise, this complex of functions which is a linear-function is just a equation, it will give a result, but can not be use for new data. ( there is no exact answer to a problem, can be very close, but never 100% )\n",
    "\n",
    "Why use ReLU? \n",
    "\n",
    "ReLU makes the activations becomes non-linearity. Think of it as randomness. Each time weights and input is randomized, and ReLU remove some of the features of a function in these functions.\n",
    "\n",
    "Andrew Ng's Machine Learning course give a much more cleared reason on why this works, with support of Math.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Just remember it is just:\n",
    "\n",
    "weights(i-1) -= weights(i).gradient * learning_rate\n",
    "\n",
    ", where i is the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "For a specific model, like Resnet, we can use pre-trained model, to train a new problem, which use the same structure of model.\n",
    "\n",
    "For example, ImageNet has a trained resnet-52 model, which classify 1000 things.\n",
    "\n",
    "We can download the cache of that model, and use that to train a new same structure of resnet-52 model, which classify 3 type of bears.\n",
    "\n",
    "The process is called Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-4.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Resnet is basically a CNN. One feature that CNN has is that for the first 1-3 layers, it is very good at finding image features. E.g. edges, shapes, lines...\n",
    "\n",
    "The deeper the layer it goes, the more complex idea of a image feature it will recognize. At 10th layer, it maybe good at recognize idea like eye-balls, furry-stuff, metal-shape, etc.\n",
    "\n",
    "The idea of fine-tuning is that, we do not need to re-train the first few layers, just remove the last few layers that recognize complex idea that specific to classifying 1000 stuff, and re-train the last few layers with new 3 bear classify's data.\n",
    "\n",
    "For example, Freeze first 2/3 of the layers, and re-init the last 1/3 of layers, and only train the last 1/3 of layers by only back-propagate into those layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-5.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-6.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-7.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-8.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Learning Rate\n",
    "\n",
    "In Fine-Tuning, after training only 1/3 of the last layers, we can un-freeze the first 2/3 of layers. But we do not want to have the first 2/3 of the layers have the same learning rate of 1/3 of layers, because those layers should be pretty much optimized already. We can actually make first 2/3 of layers worse.\n",
    "\n",
    "So we set the learning rate of first 2/3 of layers to be smaller.\n",
    "\n",
    "E.g. first 1/3 of layers has learning rate of 1e-5, 2nd of 1/3 layers has 1e-4, last has 1e-3\n",
    "\n",
    "In fastai:\n",
    "```\n",
    "model.fit( 1, 1e-3 )  # No discriminative LR\n",
    "model.fit( 1, slice(1e-3) ) # deafult 2/3 layers is one-third of last 1/3 of layers\n",
    "model.fit( 1, slice(1e-5, 1e-3) ) # 1st 1/3 layer is 1e-5, 2nd is 1e-4, 3rd is 1e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Function\n",
    "\n",
    "it is a linear function.\n",
    "\n",
    "If we just put a affine function on top of another affine function, it is still just an affine function. So we need to give some non-linearity to it. like a relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
