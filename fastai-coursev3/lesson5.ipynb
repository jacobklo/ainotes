{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lessaib 5 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs\n",
    "- Weights/parameters\n",
    "    - Random\n",
    "- Activations\n",
    "- Activation functions / nonlinearities\n",
    "- Output\n",
    "- Loss\n",
    "- Metric\n",
    "- Cross-entropy\n",
    "- Softmax\n",
    "- Fine tuning\n",
    "    - Layer deletion and random weights\n",
    "    - Freezing & unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-1.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "It is the features.\n",
    "In this case, MNIST input is handwriting pictures of 28 by 28 pixels. Sine we use each pixel as a feature, so the input shape will be 28 * 28 = 784\n",
    "\n",
    "\n",
    "It is one colume of 784 rows.\n",
    "\n",
    "If it is a color picture, which mean it may have 3 channel, so the input feature will be 28 * 28 * 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "For matrix multiplication to happen correctly, one dimention of shape has to match, so the row of the weighs is fixed to the row of inputs, which is 784\n",
    "\n",
    "Note: the input layer is probably transform into shape of [1, 784]\n",
    "\n",
    "The other dimention is you pick really. Depends on want you want to outcome activation is, can be 1, can be 5, 10, 1 million. In this picture, we picked 5.\n",
    "\n",
    "That's the part what a neural networks is, how many neurons you want in this layer.\n",
    "\n",
    "Note: the last activation, the output, should be the dimention you need. For MNIST, we want to predict number from 0 to 10, so we will have dimention shape of [ 10, 1 ]. So we should pick 10 at the last layer.\n",
    "\n",
    "This weights is initialized as random number, but also should be normalized. Check out kaming normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "The new layer of activation is the result of a matrix multication between last activations and weights is performed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-2.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "This is a function that modify each value in the activation. ( element-wise function )\n",
    "\n",
    "ReLU repalce every negative number to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat next layer\n",
    "\n",
    "The process is repeat next layer, where having a new set of weights, this time is 8.\n",
    "\n",
    "Just remember that the last layer should be 10, for the example. So your last layer of weights should second-last len(activation) and 10 column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer\n",
    "\n",
    "It is just steps, can be a step of activation functions, can be a matrix multication of another weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-3.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Layer\n",
    "\n",
    "This is simply the output, which is a activation. \n",
    "\n",
    "But most of the time, we can also have another activation function, sometimes a sigmoid function, the calculate the probability of the output. \n",
    "\n",
    "E.g. the output layer has these output for MNIST by predicting what number of this handwriting should be:\n",
    "\n",
    "```\n",
    "[[0.123], [0.232], [0.897], [0.123], [0.123], [0.232], [0.456], [0.123], [0.232], [0.345], [0.254]]\n",
    "```\n",
    "\n",
    "See that the 2nd value is high, so the neural network has predicted that the handwriting should be 2\n",
    "\n",
    "We use sigmoid function to 'cap' the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost Function\n",
    "\n",
    "It is a function to calculate how accurate the network predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squred Error\n",
    "\n",
    "For Mean-Squred Error, it is just Sum of (y-y_hat)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation theorem\n",
    "\n",
    "It is basically saying if there are big-enough data, for a enough complex of function ( think of it as a huge multi-layer neural net ), you can solve any arbitrariy high level of accuracy for that function ( the basis of machine learning ). One important thing is that we need to give some non-linearity to these functions.\n",
    "\n",
    "Otherwise, this complex of functions which is a linear-function is just a equation, it will give a result, but can not be use for new data. ( there is no exact answer to a problem, can be very close, but never 100% )\n",
    "\n",
    "Why use ReLU? \n",
    "\n",
    "ReLU makes the activations becomes non-linearity. Think of it as randomness. Each time weights and input is randomized, and ReLU remove some of the features of a function in these functions.\n",
    "\n",
    "Andrew Ng's Machine Learning course give a much more cleared reason on why this works, with support of Math.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Just remember it is just:\n",
    "\n",
    "weights(i-1) -= weights(i).gradient * learning_rate\n",
    "\n",
    ", where i is the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "For a specific model, like Resnet, we can use pre-trained model, to train a new problem, which use the same structure of model.\n",
    "\n",
    "For example, ImageNet has a trained resnet-52 model, which classify 1000 things.\n",
    "\n",
    "We can download the cache of that model, and use that to train a new same structure of resnet-52 model, which classify 3 type of bears.\n",
    "\n",
    "The process is called Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-4.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Resnet is basically a CNN. One feature that CNN has is that for the first 1-3 layers, it is very good at finding image features. E.g. edges, shapes, lines...\n",
    "\n",
    "The deeper the layer it goes, the more complex idea of a image feature it will recognize. At 10th layer, it maybe good at recognize idea like eye-balls, furry-stuff, metal-shape, etc.\n",
    "\n",
    "The idea of fine-tuning is that, we do not need to re-train the first few layers, just remove the last few layers that recognize complex idea that specific to classifying 1000 stuff, and re-train the last few layers with new 3 bear classify's data.\n",
    "\n",
    "For example, Freeze first 2/3 of the layers, and re-init the last 1/3 of layers, and only train the last 1/3 of layers by only back-propagate into those layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-5.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-6.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-7.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lesson5/lesson5-8.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Learning Rate\n",
    "\n",
    "In Fine-Tuning, after training only 1/3 of the last layers, we can un-freeze the first 2/3 of layers. But we do not want to have the first 2/3 of the layers have the same learning rate of 1/3 of layers, because those layers should be pretty much optimized already. We can actually make first 2/3 of layers worse.\n",
    "\n",
    "So we set the learning rate of first 2/3 of layers to be smaller.\n",
    "\n",
    "E.g. first 1/3 of layers has learning rate of 1e-5, 2nd of 1/3 layers has 1e-4, last has 1e-3\n",
    "\n",
    "In fastai:\n",
    "```\n",
    "model.fit( 1, 1e-3 )  # No discriminative LR\n",
    "model.fit( 1, slice(1e-3) ) # deafult 2/3 layers is one-third of last 1/3 of layers\n",
    "model.fit( 1, slice(1e-5, 1e-3) ) # 1st 1/3 layer is 1e-5, 2nd is 1e-4, 3rd is 1e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Function\n",
    "\n",
    "it is a linear function.\n",
    "\n",
    "If we just put a affine function on top of another affine function, it is still just an affine function. So we need to give some non-linearity to it. like a relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay\n",
    "\n",
    "![a](lesson5/lesson5-9.png \"\")\n",
    "\n",
    "To overcome overfitting, we can use weight decay.\n",
    "We basically penalize complexity by putting sum of all weights into the result of loss as well. But because some weights is negative, so we need to use square of each weights.\n",
    "\n",
    "![a](lesson5/lesson5-10.png \"\")\n",
    "```\n",
    "Loss = MSE(y_hat, y) + wd * sum(w^2)\n",
    "```\n",
    "Weight decay (WD) is to make sure the second part of the equation become too huge, making all weights to be zero when training.\n",
    "\n",
    "fastai has a good result when wd = 0.1, but default to 0.01\n",
    "\n",
    "```\n",
    "learn = collab_learner(data, n_factors=40, y_range=[0, 5.5], wd=1e-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weight decay in lesson5-sgd\n",
    "```\n",
    "# weight decay\n",
    "    w2 = 0.\n",
    "    for p in model.parameters(): w2 += (p**2).sum()\n",
    "    # add to regular loss\n",
    "    loss = loss_func(y_hat, y) + w2*wd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why called Weight Decay?\n",
    "\n",
    "![a](lesson5/lesson5-11.png \"\")\n",
    "\n",
    "The gradient function L(x,w), base on train rule, we take derivatives on both loss function and wd * w^2 separately. the derivatives of wd * w^2 is 2 * wd * w, which is a negative number. Hence the \"Decay\" comes from.\n",
    "\n",
    "All weight decay does is to subtracts some constant times to weights every time we do a batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization\n",
    "\n",
    "the wd * w^2 part is L2 regularization. While weight decay is after derivatives part ( 2 * wd * w). So, they are mathematically the same. ( In this case )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check graddesc.xlsm\n",
    "\n",
    "This is the excel version of performing gradient decent. It also have different loss functions methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Differencing\n",
    "\n",
    "To calculate gradient, we can use derivative, but we can also use finite differencing.\n",
    "\n",
    "You can just add a very small number to your value e as e_hat, that compare the diference between e and e_hat. The slope of e and e_hat is the de/db.\n",
    "\n",
    "![a](lesson5/lesson5-12.png \"\")\n",
    "![a](lesson5/lesson5-13.png \"\")\n",
    "\n",
    "The 0.01 in the excel is the small number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "It is to speed up the converging process.\n",
    "\n",
    "![a](lesson5/lesson5-14.png)\n",
    "\n",
    "For the gradient of this loop, we will now have this formula:\n",
    "```\n",
    "gradient(i) = gradient(i-1) * 0.9 + gradient(i) * 0.1\n",
    "```\n",
    "So, if the last gradient has the same direction of this gradient, it will speed up; while if we overshoot and the current gradient is the opposite direction, it will slow down. It will eventually go back to the right direction.\n",
    "\n",
    "0.9 and 0.1 is your choice\n",
    "\n",
    "It also works on this case as well:\n",
    "![](lesson5/lesson5-15.png)\n",
    "\n",
    "![](lesson5/lesson5-16.png)\n",
    "This is also called exponentially weighted moving average.\n",
    "\n",
    "```\n",
    "opt = optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "TODO\n",
    "\n",
    "rmpsprop is very similar to momentum, but the gradient is squared.\n",
    "the value of rmsprop is small if the gradient is small and steady.\n",
    "the value of rmsprop is big if the gradient is big or it is highly volatile.\n",
    "\n",
    "Maybe wrong\n",
    "weights - lr * gradient / sqrt(rmsprop)\n",
    "\n",
    "![](lesson5/lesson5-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "TODO\n",
    "\n",
    "Adam is momentum + RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit one cycle\n",
    "\n",
    "fastai does this for us, but basically:\n",
    "\n",
    "at the beginning of training, we do not know what direction to go, so we should have a low learning rate. \n",
    "\n",
    "but once in the middle, we know where to go, we can have a higher learning rate. \n",
    "\n",
    "In the end, we need to anneal the learning rate by lowering it, so we can converge.\n",
    "\n",
    "Momentum is opposite, if in the beginning we are going the same direction, momentum will be high.\n",
    "\n",
    "In the middle, since the learning rate is already high, we should have have a high momentum, it will be too much.\n",
    "\n",
    "at the end we can have higher momentum again, if the learning rate is too small, to speed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson5/lesson5-19.png)\n",
    "\n",
    "It basically give a very small number if it predict confidently correctly.\n",
    "\n",
    "It give a huge number if it predict confidently wrongly.\n",
    "\n",
    "It give a somewhat high number if it is uncertain about its prediction.\n",
    "\n",
    "```\n",
    "D2=-B2*LOG(C2)-(1-B2)*LOG(1-C2)\n",
    "```\n",
    "A faster way is to do a lookup\n",
    "```\n",
    "E2=-IF(B2=1,LOG(C2),LOG(1-C2))\n",
    "```\n",
    "Note: The prediction output has to add up to 1. Or it will not work\n",
    "\n",
    "E.g. pred(cat) + pred(dog) + pred(bear) = 1\n",
    "\n",
    "E.g. pred(cat) + pred(not_cat) = 1\n",
    "\n",
    "You can make sure it is 1, by using the correct activation at the end of your nerual network: softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "![](lesson5/lesson5-20.png)\n",
    "![](lesson5/lesson5-21.png)\n",
    "\n",
    "the result of all the output is adding up to 1, and all are positive but smaller than 1.\n",
    "\n",
    "First, calculate all the ouput with e to the power of that\n",
    "Then, sum all output\n",
    "Last, divide each with the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
