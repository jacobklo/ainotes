{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning from the Foundations \n",
    "[[00:00]](https://youtu.be/4u8FxNEDUeg)\n",
    "\n",
    "(aka “Impractical Deep Learning for Coders”)\n",
    "\n",
    "\n",
    "- Implement much of fastai from Foundations\n",
    "    - Basic matrix calculus; Training loops w callbacks; Custom optimizer; custom annealing; regularization; dataset, loader, and blocks\n",
    "    - Lots of layers and architectures\n",
    "    - Jupyter dev, testing and docs\n",
    "\n",
    "- Read papers, and implement them. We’ll even do some new research!\n",
    "\n",
    "- Learn object detection, seq2seq/attention, Transformer/XL, CycleGAN, Audio…\n",
    "\n",
    "- Performance: distributed training, JIT, CUDA/C++\n",
    "\n",
    "- Implement some of fastai in Swift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part 2 of the course in version 3 is different than perious version, which is more deeper advanced concepts on different papers. This version teach you how to read paper your own, and convert it to code. That's why it is called Impractical Deep Learning for coders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-1.png)\n",
    "\n",
    "[[05:17]](https://youtu.be/4u8FxNEDUeg?t=317)\n",
    "\n",
    "Part 1 of this course was top-down, so that you got the context you needed to understand, you got the motivation you needed to keep going, and you got the results that you needed to make it useful.\n",
    "\n",
    "But bottom-up is useful too. Bottom-up let's you when you've built everything from the bottom yourself, then you can see the connections between all the different things. You can see they're all variations of the same thing you know, and then you can customize rather than picking algorithm A or algorithm B. You create your own algorithm to solve your own problem doing just the things you need it to do, and then you can make sure that you know that it performs well; you can debug it, profile it, maintain it, because you understand all of the pieces. \n",
    "\n",
    "Normally when people say bottom-up in in in this world in this field they mean bottom-up with math, This course mean bottom-up with code. It's because bottom up with code means that you can experiment really deeply on every part of every bit of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-2.png)\n",
    "\n",
    "[[07:30]](https://youtu.be/4u8FxNEDUeg?t=408)\n",
    "\n",
    "Chris Lattner built LLVM, clang, and Swift.\n",
    "<br>We now have people build library that has building compiler experiences.\n",
    "Swift is a good numerical programming language that was built by somebody that really gets programming languages.\n",
    "<br><br>\n",
    "None of these languages ( Lisp, R, Python ) were built to be good at data analysis they weren't built by people that really deeply understood compilers, or kind of modern highly parallel processor.\n",
    "<br><br>\n",
    "Julia is another recommended language, but it is 10x less used, doesn't have some level of community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-3.png)\n",
    "\n",
    "[[09:42]](https://youtu.be/4u8FxNEDUeg?t=582)\n",
    "\n",
    "Swift could create code from scratch that was competitive with the fastest hand-tuned vend or linear algebra libraries now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-4.png)\n",
    "\n",
    "[[10:49]](https://youtu.be/4u8FxNEDUeg?t=649)\n",
    "\n",
    "Pytorches and pythons pros you can get stuff done right now with this amazing ecosystem fantastic documentation and tutorials, it's just a really great practical system for solving problems and to be clear Swift for tensorflow is not it's not any of those things right now.\n",
    "\n",
    "MLIR compiler, basically get Swift for tensorflow to talk to LLVM directly. So it is very fast.\n",
    "\n",
    "Python as you'll see today we almost never actually write Python code. We run code in Python that gets turned into some other language or library and that's what what it gets run and  this mismatch between what I'm trying to write and what actually gets run makes it very hard to do the kind of deep dives that we're going to do in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-5.png)\n",
    "\n",
    "[[14:50]](https://youtu.be/4u8FxNEDUeg?t=890)\n",
    "\n",
    "To recreate fastai, we're only allowed to use these bits. We are allowed to use pure Python; anything in the Python standard library; any non data science modules, so like a requests library for HTTP or whatever; we can use Pytorch but only for creating arrays, random number generation, and indexing into arrays; we can use the fastai datasets library because that's the thing that has access to like MNIST and stuff, so we don't have to worry about writing our own HTTP stuff; and we can use matplotlib so we don't have to write our own plotting library. That's it.\n",
    "\n",
    "Each time we have replicated some piece of fastai or Pytorch from the foundations we can then use the real version if we want to.\n",
    "\n",
    "What I've discovered is I started doing that is that I started actually making things a lot better than before. Because there was a whole lot of things I could have done better and so you'll find the same thing as you go along this journey. You'll find decisions that I made or the Pytorch team made or whatever. You think what if they'd made a different decision there and you can you know maybe come up with more examples of things that we could do differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-6.png)\n",
    "\n",
    "[[17:03]](https://youtu.be/4u8FxNEDUeg?t=1023)\n",
    "\n",
    "If you can create something from scratch yourself you know that you understand it and then once you've created something from scratch and you really understand it then you can tweak everything\n",
    "\n",
    "For those of you interested in going deeper into research you'll be implementing papers which means you'll be able to correlate the code that you're writing with the paper that you're reading and if you're poor mathematician, then you'll find that you'll be getting a much better understanding of papers you might otherwise have thought were beyond you and you realize that all those Greek symbols actually just map to pieces of code that you're already very familiar with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-7.png)\n",
    "\n",
    "[[18:46]](https://youtu.be/4u8FxNEDUeg?t=1126)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-8.png)\n",
    "\n",
    "If you are not familiar these concept, go back to part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-9.png)\n",
    "\n",
    "[[21:00]](https://youtu.be/4u8FxNEDUeg?t=1260)\n",
    "\n",
    "The most important thing is we're going to try and make sure that you can train really good models.\n",
    "\n",
    "There are three steps to training a really good model. Step one is to create something with way more capacity you need and basically no regularization and an overfit.\n",
    "\n",
    "So overfit means what it means that your training loss is lower than your validation loss? No, it doesn't mean that. A well fit model will almost always have training loss lower than the validation loss, overfit means you have actually personally seen your validation error getting worse.Until you see that happening you're not overfitting.\n",
    "\n",
    "Step two is reduce overfitting. Step three well I guess step three is to like visualize the inputs and outputs and stuff like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-10.png)\n",
    "\n",
    "[[22:12]](https://youtu.be/4u8FxNEDUeg?t=1332)\n",
    "\n",
    "It's basically these are the five things that you can do in order of priority. If you can get more data you should; if you can do more data augmentation you should; if you can use a more generalizable architecture you should.\n",
    "\n",
    "And then if all those things are done then you can start adding regularization like drop out or weight decay, but remember you know at that point you're reducing the effective capacity of your model. So it's less good than the first three things.\n",
    "\n",
    "And then last of all reduce the architecture complexity. And most beginners especially start with reducing the complexity of the architecture, but that should be the last thing that you try unless your architecture is so complex that it's too slow for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-11.png)\n",
    "\n",
    "[[23:10]](https://youtu.be/4u8FxNEDUeg?t=1390)\n",
    "\n",
    "We're going to be reading papers which we didn't really do in part 1 and papers look something like this, which if you're anything like me that's terrifying. and I just go that's not something that I understand but then I remember this is the atom paper and you've all seen atom implemented in one cell of Microsoft Excel that's all\n",
    "\n",
    "so a big part of reading papers especially if you're less mathematically inclined than I am is just getting past the fear of the Greek letters. it actually really helps to go and learn the Greek alphabet so you can pronounce \n",
    "\n",
    "you'll there'll be a blog post or a tutorial that does a better job of explaining the concept than the paper does so don't be afraid to go and look for those as well but do go back to the paper racks in the end the papers the one that's helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-12.png)\n",
    "[List of mathematical symbols](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n",
    "\n",
    "![](lesson8/l8-13.png)\n",
    "[Detexify](http://detexify.kirelabs.org/classify.html)\n",
    "\n",
    "[[25:20]](https://youtu.be/4u8FxNEDUeg?t=1522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lesson8/l8-14.png)\n",
    "\n",
    "[[26:10]](https://youtu.be/4u8FxNEDUeg?t=1575)\n",
    "\n",
    "we're going to try to create a pretty competent modern CNN model.\n",
    "\n",
    "First half we're going to try and create a simple fully connected Network right so it's going to have one hidden layer so we're going to start with some input do a matrix multiply, do a loss function and a forward pass, backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From 00_exports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python notebook2script.py 00_exports.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[28:10]](https://youtu.be/4u8FxNEDUeg?t=1687)\n",
    "\n",
    "A lot of very smart people have assured me that it is impossible to do effective library development in Jupiter notebooks, which is a shame because I've built a library using notebooks. I would guess I'm about two to three times more productive right\n",
    "\n",
    "we can't just create one giant notebook with our whole library, somehow we have to be able to pull out those little gems those bits of code where we think is good. we have to pull that out into a package that we reuse so in order to tell our system that here is a cell that I want you to keep.\n",
    "\n",
    "```#export``` at the top of the cell and then I have a program called ```notebook2script.py``` which goes through the notebook and finds those cells and puts them into a Python module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import generated script from exp.nb_00\n",
    "from exp.nb_00 import *\n",
    "import operator\n",
    "\n",
    "def test(a,b,cmp,cname=None):\n",
    "    if cname is None: cname=cmp.__name__\n",
    "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
    "\n",
    "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
    "\n",
    "test_eq(TEST,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```test_eq()``` should not return error, if ```exp.nb_00``` is imported correctly.\n",
    "\n",
    "There's a lot of test frameworks around but it's not always helpful to use them, called test equals which calls tests passing an A and B and operator dot equals. so if they're wrong assertion error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run tests in console:\n",
    "```\n",
    "python run_notebook.py 01_matmul.ipynb\n",
    "```\n",
    "\n",
    "we probably want to be able to run these tests somewhere other than just inside a notebook so we have a little program called run notebook top py and you pass it the name of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File : run_notebook.py\n",
    "\n",
    "import nbformat,fire\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "def run_notebook(path):\n",
    "    \"Executes notebook `path` and shows any exceptions. Useful for testing\"\n",
    "    nb = nbformat.read(open(path), as_version=nbformat.NO_CONVERT)\n",
    "    ExecutePreprocessor(timeout=600).preprocess(nb, {})\n",
    "    print('done')\n",
    "\n",
    "if __name__ == '__main__': fire.Fire(run_notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_notebook.py which is our test Runner.\n",
    "\n",
    "nbformat basically lets you execute a notebook and it prints out any errors.\n",
    "\n",
    "fire is a really neat library that lets you take any function like this one and automatically converts that into a command-line interface all right so here I've got a function called run notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File : notebook2script.py\n",
    "    \n",
    "import json,fire,re\n",
    "from pathlib import Path\n",
    "\n",
    "def is_export(cell):\n",
    "    if cell['cell_type'] != 'code': return False\n",
    "    src = cell['source']\n",
    "    if len(src) == 0 or len(src[0]) < 7: return False\n",
    "    #import pdb; pdb.set_trace()\n",
    "    return re.match(r'^\\s*#\\s*export\\s*$', src[0], re.IGNORECASE) is not None\n",
    "\n",
    "def getSortedFiles(allFiles, upTo=None):\n",
    "    '''Returns all the notebok files sorted by name.\n",
    "       allFiles = True : returns all files\n",
    "                = '*_*.ipynb' : returns this pattern\n",
    "       upTo = None : no upper limit\n",
    "            = filter : returns all files up to 'filter' included\n",
    "       The sorting optioj is important to ensure that the notebok are executed in correct order.\n",
    "    '''\n",
    "    import glob\n",
    "    ret = []\n",
    "    if (allFiles==True): ret = glob.glob('*.ipynb') # Checks both that is bool type and that is True\n",
    "    if (isinstance(allFiles,str)): ret = glob.glob(allFiles)\n",
    "    if 0==len(ret): \n",
    "        print('WARNING: No files found')\n",
    "        return ret\n",
    "    if upTo is not None: ret = [f for f in ret if str(f)<=str(upTo)]\n",
    "    return sorted(ret)\n",
    "\n",
    "def notebook2script(fname=None, allFiles=None, upTo=None):\n",
    "    '''Finds cells starting with `#export` and puts them into a new module\n",
    "       + allFiles: convert all files in the folder\n",
    "       + upTo: convert files up to specified one included\n",
    "       \n",
    "       ES: \n",
    "       notebook2script --allFiles=True   # Parse all files\n",
    "       notebook2script --allFiles=nb*   # Parse all files starting with nb*\n",
    "       notebook2script --upTo=10   # Parse all files with (name<='10')\n",
    "       notebook2script --allFiles=*_*.ipynb --upTo=10   # Parse all files with an '_' and (name<='10')\n",
    "    '''\n",
    "    # initial checks\n",
    "    if (allFiles is None) and (upTo is not None): allFiles=True # Enable allFiles if upTo is present\n",
    "    if (fname is None) and (not allFiles): print('Should provide a file name')\n",
    "    if not allFiles: notebook2scriptSingle(fname)\n",
    "    else:\n",
    "        print('Begin...')\n",
    "        [notebook2scriptSingle(f) for f in getSortedFiles(allFiles,upTo)]\n",
    "        print('...End')\n",
    "        \n",
    "        \n",
    "def notebook2scriptSingle(fname):\n",
    "    \"Finds cells starting with `#export` and puts them into a new module\"\n",
    "    fname = Path(fname)\n",
    "    fname_out = f'nb_{fname.stem.split(\"_\")[0]}.py'\n",
    "    main_dic = json.load(open(fname,'r',encoding=\"utf-8\"))\n",
    "    code_cells = [c for c in main_dic['cells'] if is_export(c)]\n",
    "    module = f'''\n",
    "#################################################\n",
    "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n",
    "#################################################\n",
    "# file to edit: dev_nb/{fname.name}\n",
    "\n",
    "'''\n",
    "    for cell in code_cells: module += ''.join(cell['source'][1:]) + '\\n\\n'\n",
    "    # remove trailing spaces\n",
    "    module = re.sub(r' +$', '', module, flags=re.MULTILINE)\n",
    "    if not (fname.parent/'exp').exists(): (fname.parent/'exp').mkdir()\n",
    "    output_path = fname.parent/'exp'/fname_out\n",
    "    open(output_path,'w').write(module[:-2])\n",
    "    print(f\"Converted {fname} to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__': fire.Fire(notebook2script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How notebook2script works\n",
    "import json\n",
    "d = json.load(open('00_exports.ipynb','r'))['cells']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note2book2script bascailly load the ipynb as json, and convert it into py format.\n",
    "\n",
    "The ```How notebook2script works``` shows ipynb is actually a json file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
