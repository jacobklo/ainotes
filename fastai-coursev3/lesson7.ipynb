{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson7-resnet-mnist.ipynb\n",
    "[Jump to lesson 7 on resnet-mnist](https://youtu.be/9spwoDYwW_I?t=127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datablock - ImageItemList - from_folder()\n",
    "```\n",
    "il = ImageList.from_folder(path, convert_mode='L')\n",
    "```\n",
    "\n",
    "convert_mode='L' is passed to the library pillow, and open the image as black and white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defaults.cmap\n",
    "\n",
    "set the default color map for fastai"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pytorch put channel first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split_by_folder\n",
    "```\n",
    "sd = il.split_by_folder(train='training', valid='testing')\n",
    "```\n",
    "you can use .no_split() if you don't want validation set.\n",
    "\n",
    "cannot skip this method.\n",
    "\n",
    "you can give the folder name on ='name' attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform on handwriting MNIST\n",
    "\n",
    "we cannot flip it, cannot rotate, it will change the meaning.\n",
    "\n",
    "we cannot zoom, it will be too fuzzy on this image size.\n",
    "\n",
    "so just do padding.\n",
    "```\n",
    "tfms = ([*rand_pad(padding=3, size=28, mode='zeros')], [])\n",
    "```\n",
    "Also, no transform on validation set ( see [] )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# not using imagenet_stats because not using pretrained model\n",
    "data = ll.databunch(bs=bs).normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of channel in CNN\n",
    "```\n",
    "conv(1, 8) # 14\n",
    "```\n",
    "why 8 channel? you pick.\n",
    "\n",
    "also, 14 is the size of the image on the layer, since it is stride=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten()\n",
    "\n",
    "The last of conv(16, 10) gives out a 3D tensor of 10 by 1 by 1.\n",
    "But the loss function only take array ( array of 10 elements of popubilities for each digits ).\n",
    "\n",
    "Flatten() remove those extra dimentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch module as function\n",
    "```\n",
    "model(xb)\n",
    "```\n",
    "We can use any pytorch.nn.module and called it as function, it will call forward() method for you. It is by design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride-2 vs Stride-1\n",
    "\n",
    "Stride-1 does not change output activations size, stride-2 reduces by half. So you can add as many stride-1 layer as you like.\n",
    "\n",
    "But it is not as simple. There are below problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming He paper\n",
    "![](lesson7/lesson7-1.png)\n",
    "Let's look at the training error of a network of 20 layers, and one with 56 layers.\n",
    "\n",
    "56 layers one have a lot more stride-1 convolutions in the middle. So the one with more parameters should seriously overfit right? So you would expect the 56 layer one to zip down to zero-ish training error pretty quickly.\n",
    "\n",
    "But this is not happen, it is actually worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming He assumtion\n",
    "\n",
    "He did not know why this problem occur, but he said if he can change the 56 layer a little bit, he can make it as good as the 20 layers one.\n",
    "\n",
    "![](lesson7/lesson7-2.png)\n",
    "\n",
    "For every 2 layer, instead of the normal output = c2(c1(x)), He change it to output = x + c2(c1(x)).\n",
    "\n",
    "In other words, if the weights is zero, the layers becomes nothing happen at all, skip connection ( no change, pass-on ). That's why it is called identity x.\n",
    "\n",
    "\n",
    "This is called a res-block. and the result is revolutionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why better result?\n",
    "![](lesson7/lesson7-3.png)\n",
    "\n",
    "The x and y is the weights' space, while z is the loss. without res-block, it is very bumpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet\n",
    "![](lesson7/lesson7-4.png)\n",
    "So basically, instaed of x + c2(c1(x)), we concatenate it.\n",
    "\n",
    "Since the dense-block is keeping the original pixel, it works well on segmentation. Because you want to reconstruct the original pixel of the picture\n",
    "\n",
    "But going deeper every layer, since we concat it, the ouput get bigger in size. so it is very memory intensive.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "check lesson3-camvid-tiramisu.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai SegmentationItemList\n",
    "\n",
    "It will automatically shows the colorcoded pixel on top of the image, where different color represent a class of object that its classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Segmentation works\n",
    "\n",
    "In order for the model to classify a particular pixel to be a pedestrian, while another pixel is a bicyclist, It needs to know what a pedestrian looks like, in a broader sense. It needs to understand that this area of pixel belongs to a pedestrian.\n",
    "\n",
    "It needs to really understand a lot about this picture before classifying each pixel.\n",
    "\n",
    "We can get it work well by pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "[Jump to lesson7 U-net lecture](https://youtu.be/9spwoDYwW_I?t=1860)\n",
    "![](lesson7/lesson7-5.png)\n",
    "\n",
    "It basically have 3 part. First part ( left side of U ) is to use maybe stride-2 conv to half-size the activations, while creating more channel ( remember, each face plane is a feature ).\n",
    "\n",
    "Seoncd part ( bottom of U ) is to stop that stride-2 conv when the activation image hit a certain size ( let say 11 by 11 ). Don't go down to 1 by 1, this is not a classifier.\n",
    "\n",
    "Thrid part ( right side of U ) is to re-construct a image by increasing the activation image size. We can do it by doing stride-half convolution, also known as a deconvolution, or transposed convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride-half Convolution - slow method\n",
    "![](lesson7/lesson7-6.png)\n",
    "The blue are inputs ( 2 by 2 ). It is padded to a imaginary 7 by 7 plane, as shown.\n",
    "\n",
    "The shadow is the 3 by 3 kernal.\n",
    "\n",
    "Performing image kernal computation, we got a 5 by 5 output ( the green part ).\n",
    "\n",
    "But, it is very slow, a lot of computation. \n",
    "\n",
    "Also, the red circle part, it has a lot of padded zero, 8 or 7 out of 9 in the shadow is zero. It waste a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride-half Convolution - better method\n",
    "![](lesson7/lesson7-7.png)\n",
    "Instead of padding, just up-sample it.\n",
    "\n",
    "It is called nearest neighbor interpolation.\n",
    "\n",
    "Another simular method called bilinear interpolation. It is kind of like a weighted average for each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy and crop\n",
    "![](lesson7/lesson7-8.png)\n",
    "Even though we can use stride-half convolution, it is still impossible to get a image activation of 28 by 28, get generate back to 572 by 572, with that fine-details ( the color-coded the shape of pedestrian so well ).\n",
    "\n",
    "There are also path ( copy and crop ), from each down-sampling on left, connected to right up-sampling. Think of it kind of like skip-connection in res-block.\n",
    "\n",
    "Those copy and crop is concatenated into the up-sampling layer ( show as blue-framed white box )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lecture on UNet fastai source code explanation\n",
    "[Jump to Lecture on Unet source code](https://youtu.be/9spwoDYwW_I?t=2682)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Restoration\n",
    "Check lesson7-superres-gan.ipynb\n",
    "\n",
    "Input with poor quality, low resolution, with words on the picutre, output with high quality, remove writing on it pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crappify\n",
    "Create poor quality photo, added random test on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai parallel()\n",
    "It run a function in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance on using pretrained model\n",
    "```\n",
    ".databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n",
    "```\n",
    "\n",
    "the imagenet_stats identify that we are using the same stats as imagenet. Because we want to use their pretrained model.\n",
    "\n",
    "The poor quality photo, that has text on it, in order to remove the test, we kind of need to know what a dog should look like under that text. That's why we use pretrain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet_learner\n",
    "```\n",
    "unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,\n",
    "                         self_attention=True, y_range=y_range, loss_func=loss_gen)\n",
    "```\n",
    "\n",
    "blur, norm_type, and self_attention is very important for this generator type of things. Reasons will explain in part 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELossFlat()\n",
    "\n",
    "normal MSELoss takes array, this take a matrix, so it is good for image input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affects on using MSELoss\n",
    "\n",
    "The result of the generated image did a good job on removing the text on the picture, but did not do a good job on making better quality, It also failed to re-create the details on the eyes.\n",
    "\n",
    "It is because the loss function we are using does not really describe what we want. The pixel differences of the original and the calculated is actually pretty small. But we are missing the texture of the cat's fur, and the eyeball too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative adversarial network\n",
    "![](lesson7/lesson7-9.png)\n",
    "A GAN tried to solve this problem by using a loss function that calls another model for you.\n",
    "\n",
    "A Critic is another model, a binary classification model. It asks the input, is it a high-res cat? or a generated cat?\n",
    "\n",
    "The Loss is now: how good are we fooling the critic?\n",
    "\n",
    "It is a back and forth process. We first train some batches on the generator, using the critic as a loss function. The generator is going to become good at fooling the critic.\n",
    "\n",
    "We then start training the critic on the newly generated images and stop the generator, the critic is going to be better.\n",
    "\n",
    "We then train generator again with newly improved critic as a loss function again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with GAN\n",
    "\n",
    "If the Generator and the Critic is not using a pretrain model, it is very hard to train from start. It is like a blind leading a blind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson7-superres-gan.ipynb #Save generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free GPU memory\n",
    "The following code can be run in notebook and you don't need to restart jupyter kernel.\n",
    "```\n",
    "learn = None\n",
    "gc.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on critic learner\n",
    "```\n",
    "loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\n",
    "def create_critic_learner(data, metrics):\n",
    "    return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd)\n",
    "```\n",
    "We basically cannot use Resnet (maybe we can with spectral norm set in it ).\n",
    "\n",
    "bascially: When doing a GAN, you need to be particularly careful that the generator and the critic can't kind of push into the same direction and like increase the weights out of control.\n",
    "\n",
    "We have to use something called spectral normalization.\n",
    "\n",
    "Reason will explain in part 2.\n",
    "\n",
    "gan_critic() will give you a binary classifier suitable for GAN.\n",
    "\n",
    "\n",
    "A GAN critic uses a slightly different way of averaging the different part of image when it does the loss, so anytime you are doing a GAN at the moment you have the wrap the function with AdaptiveLoss()\n",
    "\n",
    "\n",
    "accuracy_thresh_expand is kind of the accuracy metrics but for GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson7-superres-gan.ipynb #GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher,opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple loss function for GAN\n",
    "\n",
    "```\n",
    "weights_gen=(1.,50.)\n",
    "```\n",
    "We don't only use the critic as a loss function, otherwise the GAN could get very good at creating pictures that look like real pictures, but the picture actually have nothing to do with the original photo at all.\n",
    "\n",
    "So we also added pixel loss as well.\n",
    "\n",
    "The weights_gen I think it is passing which loss function we use more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum on GAN\n",
    "```\n",
    "betas=(0.,0.99))\n",
    "```\n",
    "GAN hates momentum. And it doesn't make sense. As we keep switching between generator and critic.\n",
    "\n",
    "0 is zero momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of GAN learner\n",
    "```\n",
    "epoch\ttrain_loss\tgen_loss\tdisc_loss\n",
    "1\t2.071352\t2.025429\t4.047686\n",
    "2\t1.996251\t1.850199\t3.652173\n",
    "3\t2.001999\t2.035176\t3.612669\n",
    "4\t1.921844\t1.931835\t3.600355\n",
    "5\t1.987216\t1.961323\t3.606629\n",
    "6\t2.022372\t2.102732\t3.609494\n",
    "7\t1.900056\t2.059208\t3.581742\n",
    "8\t1.942305\t1.965547\t3.538015\n",
    "9\t1.954079\t2.006257\t3.593008\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "The result of the losses should is meaningless. Because we are switch between generator and critic, both should get better and better. So there should always be some losses ( loss should not keep going down )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
