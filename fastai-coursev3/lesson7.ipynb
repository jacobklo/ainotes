{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lesson7-resnet-mnist.ipynb\n",
    "[Jump to lesson 7 on resnet-mnist](https://youtu.be/9spwoDYwW_I?t=127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datablock - ImageItemList - from_folder()\n",
    "```\n",
    "il = ImageList.from_folder(path, convert_mode='L')\n",
    "```\n",
    "\n",
    "convert_mode='L' is passed to the library pillow, and open the image as black and white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defaults.cmap\n",
    "\n",
    "set the default color map for fastai"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pytorch put channel first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split_by_folder\n",
    "```\n",
    "sd = il.split_by_folder(train='training', valid='testing')\n",
    "```\n",
    "you can use .no_split() if you don't want validation set.\n",
    "\n",
    "cannot skip this method.\n",
    "\n",
    "you can give the folder name on ='name' attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform on handwriting MNIST\n",
    "\n",
    "we cannot flip it, cannot rotate, it will change the meaning.\n",
    "\n",
    "we cannot zoom, it will be too fuzzy on this image size.\n",
    "\n",
    "so just do padding.\n",
    "```\n",
    "tfms = ([*rand_pad(padding=3, size=28, mode='zeros')], [])\n",
    "```\n",
    "Also, no transform on validation set ( see [] )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# not using imagenet_stats because not using pretrained model\n",
    "data = ll.databunch(bs=bs).normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of channel in CNN\n",
    "```\n",
    "conv(1, 8) # 14\n",
    "```\n",
    "why 8 channel? you pick.\n",
    "\n",
    "also, 14 is the size of the image on the layer, since it is stride=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten()\n",
    "\n",
    "The last of conv(16, 10) gives out a 3D tensor of 10 by 1 by 1.\n",
    "But the loss function only take array ( array of 10 elements of popubilities for each digits ).\n",
    "\n",
    "Flatten() remove those extra dimentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch module as function\n",
    "```\n",
    "model(xb)\n",
    "```\n",
    "We can use any pytorch.nn.module and called it as function, it will call forward() method for you. It is by design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride-2 vs Stride-1\n",
    "\n",
    "Stride-1 does not change output activations size, stride-2 reduces by half. So you can add as many stride-1 layer as you like.\n",
    "\n",
    "But it is not as simple. There are below problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming He paper\n",
    "![](lesson7/lesson7-1.png)\n",
    "Let's look at the training error of a network of 20 layers, and one with 56 layers.\n",
    "\n",
    "56 layers one have a lot more stride-1 convolutions in the middle. So the one with more parameters should seriously overfit right? So you would expect the 56 layer one to zip down to zero-ish training error pretty quickly.\n",
    "\n",
    "But this is not happen, it is actually worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming He assumtion\n",
    "\n",
    "He did not know why this problem occur, but he said if he can change the 56 layer a little bit, he can make it as good as the 20 layers one.\n",
    "\n",
    "![](lesson7/lesson7-2.png)\n",
    "\n",
    "For every 2 layer, instead of the normal output = c2(c1(x)), He change it to output = x + c2(c1(x)).\n",
    "\n",
    "In other words, if the weights is zero, the layers becomes nothing happen at all, skip connection ( no change, pass-on ). That's why it is called identity x.\n",
    "\n",
    "\n",
    "This is called a res-block. and the result is revolutionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why better result?\n",
    "![](lesson7/lesson7-3.png)\n",
    "\n",
    "The x and y is the weights' space, while z is the loss. without res-block, it is very bumpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet\n",
    "![](lesson7/lesson7-4.png)\n",
    "So basically, instaed of x + c2(c1(x)), we concatenate it.\n",
    "\n",
    "Since the dense-block is keeping the original pixel, it works well on segmentation. Because you want to reconstruct the original pixel of the picture\n",
    "\n",
    "But going deeper every layer, since we concat it, the ouput get bigger in size. so it is very memory intensive.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "check lesson3-camvid-tiramisu.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai SegmentationItemList\n",
    "\n",
    "It will automatically shows the colorcoded pixel on top of the image, where different color represent a class of object that its classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Segmentation works\n",
    "\n",
    "In order for the model to classify a particular pixel to be a pedestrian, while another pixel is a bicyclist, It needs to know what a pedestrian looks like, in a broader sense. It needs to understand that this area of pixel belongs to a pedestrian.\n",
    "\n",
    "It needs to really understand a lot about this picture before classifying each pixel.\n",
    "\n",
    "We can get it work well by pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "[Jump to lesson7 U-net lecture](https://youtu.be/9spwoDYwW_I?t=1860)\n",
    "![](lesson7/lesson7-5.png)\n",
    "\n",
    "It basically have 3 part. First part ( left side of U ) is to use maybe stride-2 conv to half-size the activations, while creating more channel ( remember, each face plane is a feature ).\n",
    "\n",
    "Seoncd part ( bottom of U ) is to stop that stride-2 conv when the activation image hit a certain size ( let say 11 by 11 ). Don't go down to 1 by 1, this is not a classifier.\n",
    "\n",
    "Thrid part ( right side of U ) is to re-construct a image by increasing the activation image size. We can do it by doing stride-half convolution, also known as a deconvolution, or transposed convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride-half Convolution - slow method\n",
    "![](lesson7/lesson7-6.png)\n",
    "The blue are inputs ( 2 by 2 ). It is padded to a imaginary 7 by 7 plane, as shown.\n",
    "\n",
    "The shadow is the 3 by 3 kernal.\n",
    "\n",
    "Performing image kernal computation, we got a 5 by 5 output ( the green part ).\n",
    "\n",
    "But, it is very slow, a lot of computation. \n",
    "\n",
    "Also, the red circle part, it has a lot of padded zero, 8 or 7 out of 9 in the shadow is zero. It waste a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride-half Convolution - better method\n",
    "![](lesson7/lesson7-7.png)\n",
    "Instead of padding, just up-sample it.\n",
    "\n",
    "It is called nearest neighbor interpolation.\n",
    "\n",
    "Another simular method called bilinear interpolation. It is kind of like a weighted average for each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy and crop\n",
    "![](lesson7/lesson7-8.png)\n",
    "Even though we can use stride-half convolution, it is still impossible to get a image activation of 28 by 28, get generate back to 572 by 572, with that fine-details ( the color-coded the shape of pedestrian so well ).\n",
    "\n",
    "There are also path ( copy and crop ), from each down-sampling on left, connected to right up-sampling. Think of it kind of like skip-connection in res-block.\n",
    "\n",
    "Those copy and crop is concatenated into the up-sampling layer ( show as blue-framed white box )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lecture on UNet fastai source code explanation\n",
    "[Jump to Lecture on Unet source code](https://youtu.be/9spwoDYwW_I?t=2682)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
